---
title: "Gradiente"
author: "Armando"
date: "6/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Cargamos librerías

```{r}

library(readr)
library(ISLR)
library(MASS)
library(dplyr)
library (here)

```

## Cargamos los datos

```{r}

X4_1_data <- read_csv(here("data/4_1_data.csv"))

```

## Visualizamos los datos

Tenemos tres variables formadas por 100 observaciones.
"label" indica si un alumno ha sido admitido (1) o no (0)
"score 1" y "score2" son las notas de la primera y segunda prueba respectivamente

```{r}

# Vemos los nombres de las variables

names(X4_1_data)

# Cambiamos los nombres al dataset y a sus variables (para evitar errores con los guiones)

datos <-X4_1_data

datos <- rename(datos,"score1" = `score-1`)
datos <- rename(datos,"score2" = `score-2`)

# Vemos los datos

summary (datos)

```

## Graficamos los datos

```{r}

# Gráfico de dispersión

##### Vemos las notas de las dos pruebas, y en color rojo los alumnos admitidos, y en negro los no admitidos.

plot(datos$score1, datos$score2, col = as.factor(datos$label), xlab = "score1", ylab = "score2")

# Histograma

##### Vemos los histogramas de las dos pruebas y de los alumnos admitidos y no admitidos

hist(datos$score1)
hist(datos$score2)
hist(datos$label)

```

## Creamos datos de entrenamiento y de test

```{r}

# Establecemos una semilla
set.seed(1996)

# Contamos el número de filas
n <- nrow(datos)

# Creamos un split para train y para test aleatorio. Los datos de training serán el 70% y los de test el 30%
train_aleatorio <- sample(1:n, 0.70*n) 
datos.train <- datos[train_aleatorio,]
datos.test <- datos[-train_aleatorio,]

```

```{r}

# Train
x.train <- data.frame(rep(1,70), datos.train$score1, datos.train$score2) 
x <- as.matrix(x.train) #Esto sería x train
y <- as.matrix(datos.train$label) #Esto sería y train

# Test
x.test <- data.frame(rep(1,30), datos.test$score1, datos.test$score2) 
x.test <- as.matrix(x.test) 
y.test <- as.matrix(datos.test$label)

```

## Creamos la función Sigmoide

```{r}

Sigmoid <- function(x) 
  1 / (1 + exp(-x))

```

#Funcion de costes

Trata de determinar el error entre el valor estimado y el valor real, tratando de optimizar los parámetros. En este caso, representa los errores que se cometen al estimar la variable dependiente con los coeficientes beta estimados respecto del valor real.
El objetivo es encontrar los valores de los betas óptimos que minimicen el valor de la función de costes

```{r}

# Creamos la función

funcionCostes <- function(parametros, X, Y) {
  n <- nrow(X)
  g <- Sigmoid(X %*% parametros)
  J <- (1/n) * sum((-Y * log(g)) - ((1 - Y) * log(1 - g)))
  return(J)
}

```

```{r}

# Coste de inicio
### Tomando como valor inicial de los parámetros (beta) cero, se calcula el coste inicial. El objetivo es reducir ese coste.

parametros <- rep(0, ncol(x))

# Coste máximo

coste_inicial = funcionCostes(parametros, x, y)
coste_inicial

# Hacemos que nos devuelva el coste inicial de forma más limpia

print(paste("El coste inicial de la funcion es: ", convergence <- c(funcionCostes(parametros, x, y)), sep = ""))

```

## Cálculo del número óptimo de iteraciones

Crearemos una función para obtener los parámetros óptimos

```{r}

TestGradientDescent <- function(iterations = 1200, X, Y) {
  parametros <- rep(0, ncol(X)
                    )
  print(paste("Función inicial de costes: ", 
              convergence <- c(funcionCostes(parametros, X, Y)), sep = ""))
  parametros_optimizacion <- optim(par = parametros, fn = funcionCostes, X = X, Y = Y, 
                                   control = list(maxit = iterations))
  #Seleccionamos los parámetros
  parametros <- parametros_optimizacion$par
  
  # Chequeamos la evolución
  print(paste("Valor final de la función de costes: ", 
              convergence <- c(funcionCostes(parametros, X, Y)), sep = ""))

 return(parametros) 
}

```

```{r}

# Ejecutamos la función para nuestros valores

parametros_optimos <-TestGradientDescent(X = x, Y= y)
parametros_optimos

probabilidades <- Sigmoid((x.test %*% parametros_optimos)) 
probabilidades

```

```{r}

# Hacemos el cut of

probabilidades[probabilidades >= 0.5] <- 1
probabilidades[probabilidades< 0.5] <- 0
probabilidades

y.test

```

## Matriz de confusión

```{r}

#Creamo la matriz de confusión

table(y.test, probabilidades, dnn=c("Real", "Prediccion"))

```

```{r}

# Calculamos el accuracy

accuracy<-100*sum(diag(table(y.test, probabilidades)))/sum(table(y.test, probabilidades)) 
accuracy

```

## Iteraciones individuales

```{r}

# Sacamos el error del modelo para cada iteración

TestGradientDescent <- function(iterations = 1200, X, Y) {
  parametros <- rep(0, ncol(X))
  errores <- NULL
  for (iteracion in 1:iterations) {
    parametros_optimizacion <- optim(par = parametros, fn = funcionCostes, X = X, Y = Y, 
                                   control = list(maxit = iteracion))
    errores[iteracion]<- parametros_optimizacion$value
  }
 return(errores) 
}

# Asignamos valores

ejeY <- TestGradientDescent(400, X = x.test, Y = y.test)
ejex <- 1:400

# Representamos gráficamente el error medio para cada iteración

plot(x = ejex, y=ejeY)

```

## Explorar la función "Optium"

```{r}

#Vemos qué argumentos tiene esta función
args(optim)

##### Existen diversos métodos para optimizar la función y su convergencia usando la funcion "optim". Estos métodos son: method = "Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"

##### En la salida de la función se explica si han convergido (valor 0) o no (valor 1)

```

Exploraremos ahora el método BFGS:

Hace uso tanto del gradiente como de una aproximación a la inversa de la matriz hessiana de la función, esto para hacer una aproximación al cálculo de la segunda derivada. Tiene el problema de que es costoso computacionalmente para funciones de muchas variables. Es adecuado para funciones no lineales de varias variables.

```{r}

parametros_optimizados1 <- optim(par = parametros, fn = funcionCostes, X = x, Y = y, method = "BFGS",lower=-Inf,upper=Inf,control = list(maxit = 60), hessian=TRUE) 

parametros_optimizados1

```

